\documentclass[12pt, titlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{ {images/} }

\usepackage[english]{babel}
\usepackage[nottoc]{tocbibind}

\usepackage{hyperref}
\usepackage[left=3cm,right=3cm,top=2cm,bottom=2cm]{geometry}

\usepackage{bbm}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{exmp}[thm]{Example}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\title{Fourier Analysis: \\ Some results and applications}
\author{Sayantan Khan}
\date{July 2016}

\newcommand{\vep}{\varepsilon}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\renewcommand{\Re}{\mathrm{Re}}
\newcommand{\znz}{\mathbb{Z}/N\mathbb{Z}}
\newcommand{\iprod}[2]{\langle #1, #2 \rangle}
\newcommand{\indi}{\mathbbm{1}}

\begin{document}
\maketitle

\tableofcontents

\newpage

\section{Convergence results for Fourier series}

\subsection{A historical perspective}
Fourier series made their first appearance in mathematics through the way of physics, as is the norm with a lot of classical analysis. In particular, they turned up in the analysis of the following differential equation
\begin{align}
    \frac{\partial^2 u}{\partial r^2} + \frac{1}{r} \frac{\partial u}{\partial r} + \frac{1}{r^2} \frac{\partial^2 u}{\partial \theta^2} &= 0 \label{eq:1}
\end{align}
on the unit disc, with the following boundary conditions
\begin{align*}
    u(1, \theta) = f(\theta)
\end{align*}
where $f$ is an arbitrary continuous function on $[-\pi, \pi]$ and $f(-\pi) = f(\pi)$.

Equation \ref{eq:1} is what is called the Laplace equation, which describes the temperature $u$ on a conducting surface at steady state. This naturally raises the question: What would the temperature profile on a disk look like at steady state if the boundary was maintained at some temperature $f(\theta)$. That is precisely what the described differential equation along with the boundary conditions tries to answer.

Once one makes the simplifying assumption that the solution is separable, i.e.
\begin{align*}
    u(r, \theta) = a(r) \cdot b(\theta)
\end{align*}
equation \ref{eq:1} is easy to solve, and one sees that it has solutions of the form
\begin{align*}
    u_n(r, \theta) = r^n e^{in\theta}
\end{align*}
for all integers $n$. And finite linear combinations of the solutions $u_n$ are also solutions to the differential equation \ref{eq:1}. But there is one problem. Not every continuous function $f$ on $[-\pi, \pi]$ is a finite linear combination of $e^{in\theta}$, e.g. $|x|$. Leaving that problem aside for now, one notes that if $f$ is of the form
\begin{align*}
    f(\theta) = \sum_{n = -N}^{N} \widehat{f}(n) e^{in\theta}
\end{align*}
then $\widehat{f}(n)$ is can be determined in the following manner
\begin{align}
    \widehat{f}(n) = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) e^{-int} dt \label{eq:2}
\end{align}
Here $\widehat{f}(n)$ is called the $n$\textsuperscript{th} Fourier coefficient of $f$. Since we assumed that $f$ is a finite linear combination of $e^{-in\theta}$, that means it has only finitely many non-zero coefficients. Contrariwise, continuous functions that are not finite linear combinations of $e^{in\theta}$ must have infinitely many non-zero Fourier coefficients. Could it perhaps be that $f$ is an \emph{infinite} linear combination of $e^{in\theta}$? To be more precise, does the following sequence converge to $f$ (in an appropriate sense) as $N$ goes to $\infty$?
\begin{align*}
    S_N(\theta) = \sum_{n = -N}^{N} \widehat{f}(n) e^{in\theta}
\end{align*}

The best we can hope for is for the sequence $S_N$ to converge uniformly to the function $f$, but as we shall see, that is not actually true, i.e. there are continuous functions whose Fourier series diverges for some point in the domain. The next step would be either weakening the mode of convergence, i.e. instead of expecting convergence in the $L^{\infty}$ norm, one could expect convergence in weaker norms such as $L^1$ or $L^2$, or even Cesàro convergence. Another possible way to go about would be to strengthen the conditions on the function, i.e. forcing the function to be $C^1$, or even absolutely continuous ensures that the sequence $S_N$ converges uniformly to $f$.

\subsection{Fourier series of continuous functions are Cesàro summable}
Before we go on to show that Fourier series converge under weaker notions of convergence, we'll develop a technique that can be used more generally to show certain sequences of functions converge uniformly. Furthermore, in all the sections that are to follow, we'll be working with continuous complex valued functions on the circle $T$, which is defined as the space $\mathbb{R}$ quotiented with the equivalence relation $\sim$ defined as
\begin{align*}
    a \sim b \iff (a-b) = 2n\pi,\ n \in \mathbb{Z}
\end{align*}

\subsubsection{Convolution}
For two continuous functions $f$ and $g$ on $T$, we can define a binary operation $\ast$
\begin{align*}
    (f \ast g)(x) = \int_{-\pi}^{\pi} f(x-t) g(t) dt
\end{align*}
It follows that the operation is commutative (substitution of variables), associative (again substitution of variable), and distributive (integration is a linear operator). It also follows that $f \ast g$ is continuous since $f$ is uniformly continuous. It's not too hard to show that this operation has no identity element (It can be done by showing that if such a function existed, it would take on the value $0$ for every point in its domain except $0$. But then its integral would also be $0$. Contradiction!).
%\begin{prop}
%    There exists no continuous function $g$ on $T$ such that for all continuous functions $f$
%    \begin{align*}
%        f \ast g = f
%    \end{align*}
%\end{prop}
%
%\begin{proof}
%    We will prove the result by showing that if such a $g$ existed, then $g(x) = 0$ for $x \neq 0$. But that would mean $g$ is non-zero on a set of measure $0$, hence $\int_T f(x-t) g(t) dt = 0$ for all $f$, which means $g$ is not the identity, hence a contradiction.
%    
%    Assume we have a continuous function $g$ such that for all continuous $f$, $f \ast g = f$. Pick any non-zero $x_0$. We claim that $g(x_0)$ must be $0$. If it's not $0$, then without loss of generality, let $g(x_0) = \vep > 0$. That means there exists a $\delta_0$ such that for all $x \in (x_0 - \delta_0, x+ \delta_0)$, $g(x) > \frac{\vep}{2}$. Let $\delta = \min(\delta_0, |x_0|)$.
%    
%    Now define a continuous function $f$ in the following manner:
%    \begin{align*}
%        f(-x) =
%        \begin{cases} 
%        \hfill 1    \hfill & \text{$x \in (x_0 - \frac{\delta}{2}, x_0 + \frac{\delta}{2})$ } \\
%        \hfill \frac{2(x - (x_0-\delta))}{\delta} \hfill & \text{ $x \in \left( x_0 - \delta, x_0 - \frac{\delta}{2} \right]$ } \\
%        \hfill \frac{2((x_0+\delta) - x)}{\delta} \hfill & \text{ $x \in \left[ x_0 + \frac{\delta}{2}, x_0 + \delta \right)$ } \\
%        \hfill 0 \hfill & \text{otherwise}
%        \end{cases}
%    \end{align*}
%    It's clear that $f(0) = 0$.
%    \begin{align*}
%        f(0) &= (f \ast g)(0) \\
%             &= \int_{-\pi}^{\pi} f(-t) g(t) dt \\
%             &\geq \int_{x - \frac{\delta}{2}}^{x + \frac{\delta}{2}} f(-t) g(t) dt \\
%             &\geq \frac{\vep \delta}{2} \\
%             &> 0
%    \end{align*}
%    But $f(0) = 0$, and $0 \not > 0$. We have a contradiction, which means $g(x) = 0$ for all $x \neq 0$. This completes the proof.
%\end{proof}

\subsubsection{Dirac sequences}
We saw that there is no continuous function which acts as an identity for $\ast$ operation. However, there do exist sequences of functions, such that the sequence $\{f \ast g_n\}$ converges uniformly to $f$ as $n$ goes to infinity. Such a sequence $\{g_n\}$ is called a Dirac sequence. The formal definition of a Dirac sequence is the following
\begin{defn}
    A sequence of a continuous functions $\{g_n\}$ is called a Dirac sequence if it satisfies the following conditions
    \begin{enumerate}
        \item $g_n(x) \geq 0$ for all $n \in \mathbb{N}$ and all $x \in T$.
        \item $g_n(x) = g_n(-x)$ for all $n \in \mathbb{N}$ and all $x \in T$.
        \item $\displaystyle \int_{-\pi}^{\pi} g_n(t) dt = 1$ for all $n \in \mathbb{N}$.
        \item For all $\vep >0$ and $\delta > 0$, there exists an $N$ such that for all $n > N$,
        \begin{align*}
            \int_{-\pi}^{-\delta} g_n(t) dt + \int_{\delta}^{\pi} g_n(t) dt < \vep
        \end{align*}
    \end{enumerate}
\end{defn}
Following from the definition, one gets a useful theorem
\begin{thm}\label{th:1}
    If $\{g_n\}$ is a Dirac sequence, then for all continuous functions $f$, the sequence $\{f \ast g_n\}$ converges uniformly to $f$.
\end{thm}

\begin{proof}
    Pick any $\vep > 0$. We need to show there exists an $N$ such that for all $n > N$, $\norm{(f \ast g_n) - f}_\infty < \vep$. Pick a particular continuous function $f$. Let $M$ be the maximum of $|f|$ on $T$. Pick $\vep_1 > 0$ and $\vep_2 > 0$ such that
    \begin{align}
        \vep_1 \vep_2 + \vep_2 + 2M\vep_1 < \vep \label{eq:3}
    \end{align}
    Since $f$ is uniformly continuous on $T$, pick a $\delta$ such that $|x-y| < \delta$ implies $|f(x) - f(y)| < \vep_2$.
    
    Now pick an $N$ such that for all $n > N$
    \begin{align*}
        \int_{-\pi}^{-\delta} g_n(t) dt + \int_{\delta}^{\pi} g_n(t) dt < \vep_1
    \end{align*}
    This would imply for all $n > N$
    \begin{align*}
        \int_{-\delta}^{\delta} g_n(t) dt > 1 - \vep_1
    \end{align*}
    
    Now consider $(f \ast g_n)(x_0)$ for some $x \in T$.
    \begin{align*}
        (f \ast g_n)(x_0) &= \int_{-\pi}^{\pi} f(x_0-t) g_n(t) dt \\
        &= \left( \int_{-\pi}^{-\delta} f(x_0-t) g_n(t) dt + \int_{\delta}^{\pi} f(x_0-t) g_n(t) dt \right) + \left( \int_{-\delta}^{\delta} f(x_0-t) g_n(t) dt \right)
    \end{align*}
    Let's analyse the two terms separately. Since $-M \leq f(x_0-t) \leq M$, we can bound the first term as
    \begin{align*}
        -M\vep_1 \leq \int_{-\pi}^{-\delta} f(x_0-t) g_n(t) dt + \int_{\delta}^{\pi} f(x_0-t) g_n(t) dt \leq M\vep_1
    \end{align*}
    Similarly, for all $x$ in the interval $(x_0 -\delta, x_0 + \delta)$
    \begin{align*}
        f(x_0) - \vep_2 \leq f(x) \leq f(x_0) + \vep_2
    \end{align*}
    This lets us bound the second term in the following manner
    \begin{align*}
        (1- \vep_1)(f(x_0) - \vep_2) \leq \int_{-\delta}^{\delta} f(x_0 - t)g_n(t) dt \leq f(x_0) + \vep_2
    \end{align*}
    This gives us a complete bound on $(f \ast g_n)(x_0)$.
    \begin{align*}
        -M\vep_1 + (1 - \vep_1)(f(x_0) - \vep_2) \leq (f \ast g_n)(x_0) \leq M\vep_1 + (f(x_0) + \vep_2)
    \end{align*}
    Using the triangle inequality and the fact that $|f(x_0)| < M$, we get
    \begin{align*}
        f(x_0) - (\vep_1 \vep_2 + \vep_2 + 2M\vep_1) \leq (f \ast g_n)(x_0) \leq f(x_0) + (\vep_1 \vep_2 + \vep_2 + 2M\vep_1)
    \end{align*}
    But from inequality \ref{eq:3}, we get
    \begin{align*}
        f(x_0) - \vep \leq (f \ast g_n)(x_0) \leq f(x_0) + \vep
    \end{align*}
    This shows that $\{f \ast g_n\}$ converges uniformly to $f$ for all continuous $f$.
\end{proof}

This is a useful result because this will let us deal with the question of convergence of the partial Fourier series of some continuous function $f$. One can write the $n$\textsuperscript{th} partial Fourier series as the convolution of $f$ with some function $g_n$, called a kernel, and if one shows that the kernels $g_n$ form a Dirac sequence, then the Fourier series also converges.

\subsubsection{Cesàro summability}
We mentioned that for general continuous functions, their Fourier series need not converge uniformly, or even pointwise, to the function; weakening the notion of convergence lets the conjecture go through. A weaker notion of convergence for infinite sums is the notion of Cesàro summability.
\begin{defn}
    A sequence $\{x_n\}$ is said to be Cesàro summable if the following sequence $\{\sigma_n\}$ converges
    \begin{align*}
        \sigma_n = \frac{\sum_{k=1}^{n} s_k }{n}
    \end{align*}
    where $s_k$ is the $k$\textsuperscript{th} partial sum of the sequence $\{x_n\}$. If the sequence $\{ \sigma_n \}$ converges to $L$, then $L$ is called the Cesàro sum of $\{x_n\}$.
\end{defn}

It's easy to see that if the series $\displaystyle \sum_{k=1}^{n} x_n$ converges, then the Cesàro sums also converge to the same limit. To show it is a strictly weaker notion of convergence, consider the sequence $x_n = (-1)^n$. Clearly, the partial sums of $x_n$ do not converge, but the Cesàro sums do converge to $\frac{1}{2}$.

\subsubsection{The Fejér kernel}
As outlined in a previous subsection, one would like to find functions $g_N$ such that
\begin{align*}
    (f \ast g_N)(\theta) = \sum_{n = -N}^{N} \widehat{f}(n) e^{in\theta}
\end{align*}
where $\widehat{f}(n)$ is the $n$\textsuperscript{th} Fourier coefficient of $f$. Rewriting the above sum, we get
\begin{align*}
    \sum_{n = -N}^{N} \widehat{f}(n) e^{in\theta} = \sum_{n = -N}^{N} \left( \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) e^{-int} dt \right) e^{in\theta}
\end{align*}
Since this is a finite sum, we can exchange the sum and the integral to get
\begin{align*}
    \sum_{n = -N}^{N} \widehat{f}(n) e^{in\theta} = \int_{-\pi}^{\pi} \left( \frac{1}{2\pi} \sum_{n = -N}^{N} e^{in(\theta - t)} \right) f(t) dt
\end{align*}

Summing up the geometric series, we get
\begin{align*}
    \frac{1}{2\pi} \sum_{n = -N}^{N} e^{int} = \frac{1}{2\pi} \frac{\sin\left( Nt + \frac{t}{2} \right)}{\sin\left( \frac{t}{2} \right)}
\end{align*}

The function $g_N$ we wanted was is this function
\begin{align}
    g_N(t) = \frac{1}{2\pi} \frac{\sin\left( Nt + \frac{t}{2} \right)}{\sin\left( \frac{t}{2} \right)} \label{eq:5}
\end{align}
$g_N$ certainly is even, and its integral over $T$ is $1$, but it is not non-negative everywhere, hence it fails to form a Dirac sequence. Let's look at the Cesàro partial sums instead, and concentrates on the kernel corresponding to those, which are called Fejér kernels. Clearly, the kernel corresponding to the $N$\textsuperscript{th} Cesàro sum would be the following
\begin{align*}
    F_N = \frac{\sum_{n=1}^{N} g_n}{N} 
\end{align*}
Summing up the geometric progression once again, we get the following closed form expression for $F_N$
\begin{align*}
    F_N(t) &= \frac{1}{2N\pi} \frac{\sin^2 \left( \frac{Nt}{2} \right)}{\sin^2 \left( \frac{t}{2} \right)} \\
    & = \frac{1}{2N\pi} \frac{1 - \cos(nt)}{1 - \cos(t)}
\end{align*}
It follows that $F_N$ is even and its integral over $T$ is $1$. Furthermore, it is non-negative, and finally we have the following inequality for $0 < \delta < |t| \leq \pi$
\begin{align*}
    \frac{1}{2N\pi} \frac{1 - \cos(nt)}{1 - \cos(t)} &\leq \frac{1}{2N\pi} \frac{2}{1 - \cos(t)} \\
    &\leq \frac{1}{N\pi} \frac{1}{1 - \cos(\delta)}
\end{align*}
This shows that $\{F_N\}$ satisfies all the conditions required to be a Dirac sequence. This easily leads to the following result.

\begin{thm}[Fejér's Theorem] \label{th:2}
    The Fourier series of a continuous function is Cesàro summable, and the partial Cesàro sums converge uniformly to $f$.
\end{thm}

\begin{proof}
    The Fejér kernels form a Dirac sequence. Now use the result of theorem \ref{th:1}.
\end{proof}

This result gives us two things: it tells us that the exponential polynomials (linear combinations of $e^{in\theta}$) are dense in $C(T)$, and for a given $f$ in $C(T)$, it gives us an \emph{explicit} sequence of polynomials which converge uniformly to $f$. The first result is not that impressive, since it also follows from Stone-Weierstrass theorem (exponential polynomials form a subalgebra of $C(T)$ containing a non-zero constant function and it separates points), but the Stone-Weierstrass theorem does not give an explicit sequence, which this result does.

\subsection{Sufficient conditions for convergence of Fourier series}
Now that we've established the Fourier series of continuous functions are Cesàro summable, which is a notion of convergence strictly weaker than uniform convergence, we'll proceed in the other direction, which involves strengthening conditions on the function $f$.

Clearly, if the Fourier coefficients of $f$ are absolutely convergent, then that would mean the Fourier series also converges uniformly (triangle inequality). While this is rather strong condition, this does give us an idea to look at the size of Fourier coefficients in order to determine convergence. This leads us to a lemma:
%\begin{lem}
%    If $f$ is a continuous function, then
%    \begin{align*}
%        \lim_{|n| \to \infty} \widehat{f}(n) = 0
%    \end{align*}
%\end{lem}
%
%\begin{proof}
%    First step in the proof will be showing that if $f$ and $g$ are two continuous functions such that $\norm{f-g}_{\infty} < \vep$, then $|\widehat{f-g}(n)| < 2\pi \vep$. This follows from the definition of $\widehat{f-g}(n)$:
%    \begin{align*}
%        |\widehat{f-g}(n)| &= \left| \int_{-\pi}^{\pi} (f-g)(t) e^{-int} dt \right| \\
%        &\leq \int_{-\pi}^{\pi} |(f-g)(t)| \cdot |e^{-int}| dt \\
%        &< \vep \int_{-\pi}^{\pi} |e^{-int}| dt \\
%        &= 2\pi \vep
%    \end{align*}
%    
%    Pick an $\vep > 0$, and using theorem \ref{th:2}, get an exponential polynomial $g$ such that $\norm{f-g}_{\infty} < \frac{\vep}{2\pi}$. This means for all $n$, $|\widehat{f-g}(n)| < \vep$. Let the degree of $g$ be $k$. Then for $|n| > k$, $\widehat{g}(n) = 0$. That means for $|n| > k$, $|\widehat{f}(n)| = |\widehat{f-g}(n)| < \vep$. This completes the proof.
%\end{proof}
%
%This lemma tells us the that the Fourier coefficient $\widehat{f}(n)$ is $o(1)$. This is a much weaker condition than the Fourier sums being absolutely convergent, since there exist $o(1)$ sequences whose sums are not absolutely convergent, or even convergent, e.g. the harmonic series.
%
%Furthermore, we can show that Fourier coefficients of smoother functions decay faster. 
\begin{lem}
    If $f \in C^1(T)$, then $\widehat{f}(n)$ is in $o\left( \frac{1}{n} \right)$.
\end{lem}

\begin{proof}
    Consider the Fourier coefficient of $f'$, which is a continuous function.
    \begin{align*}
        \widehat{f'}(n) &= \int_{-\pi}^{\pi} f'(t) e^{-int} dt \\
        &= \left[e^{-int} f(t) \right]_{-\pi}^{\pi} + in \int_{-\pi}^{\pi} f(t) e^{-int} dt &&\text{(Integration by parts)} \\
        &= in\widehat{f}(n)
    \end{align*}
    This means 
    \begin{align*}
        \left| \widehat{f}(n) \right| = \left| \frac{\widehat{f'}(n)}{n} \right|
    \end{align*}
    Since $\widehat{f'}(n)$ is in $o(1)$, that means $\widehat{f}(n)$ is in $o\left( \frac{1}{n} \right)$.
\end{proof}
This leads to the following corollary.

\begin{cor}
    If $f \in C^k(T)$, then $\widehat{f}(n)$ is in $o \left( \frac{1}{n^k} \right)$.
\end{cor}

From the above result, we get that if $f \in C^2(T)$, then $\widehat{f}(n)$ is in $o\left( \frac{1}{n^2} \right)$, which means the Fourier sums are absolutely convergent, and the Fourier series converges uniformly to $f$.
%We have the following theorem, as a consequence.
%
%\begin{thm}
%    If $f \in C^2(T)$, then the Fourier series of $f$ converges uniformly to $f$.
%\end{thm}
%
%\begin{proof}
%    $\widehat{f}(n)$ is in $o \left( \frac{1}{n^2} \right)$. $\sum \frac{1}{n^2}$ is absolutely convergent. Hence the Fourier series converges. 
%\end{proof}

The final result of this section will be that if $f$ is continuous and $\widehat{f}(n)$ is $O\left( \frac{1}{n} \right)$, the Fourier series converges uniformly to $f$. In particular, this will give us that the Fourier series of $C^1$ converge uniformly since the Fourier coefficients are in $o\left( \frac{1}{n} \right)$.

\begin{thm}
    If $f$ is a continuous function and $\widehat{f}(n)$ is $O\left( \frac{1}{n} \right)$, then the partial Fourier sums $S_n(f)$ converge uniformly to $f$ on $T$. \cite{bhatia}
\end{thm}

\begin{proof}
%    What we need to show in the proof is that the partial Fourier sums $S_n(f)$, defined as
%    \begin{align*}
%        S_n(f)(\theta) = \sum_{k = -n}^{n} \widehat{f}(k) e^{ik\theta}
%    \end{align*}
%    converge uniformly to the function $f$, if $f$ is continuous, and $\widehat{f}(k)$ is $O\left( \frac{1}{k} \right)$. What we do know is the the partial Cesàro sums $\sigma_n(f)$, defined as
%    \begin{align*}
%        \sigma_n(f)(\theta) = \frac{\sum_{k=1}^{n} S_k(\theta)}{n}
%    \end{align*}
%    do converge uniformly to $f$. 
    What we will do in this proof is create an intermediate series $I_{(n,k)}$, between $S_n$ and $\sigma_n$ (the partial Cesàro sums), such  that $I_{(n,k)}(f)$ converges uniformly to $f$ as $n$ goes to $\infty$ for large enough $k$, and the distance between $I_{(n,k)}(f)$ and $S_m(f)$, where $kn \leq m < (k+1)n$, is less than $\frac{A}{k}$ for some constant $A$.
    
    Define the series $I_{(n,k)}$ as
    \begin{align}
        I_{n,k}(f)(\theta) = \left( k+1 + \frac{1}{n} \right)\sigma_{(k+1)n + 1}(f)(\theta) - \left( k + \frac{1}{n} \right)\sigma_{kn + 1}(f)(\theta) \label{eq:4}
    \end{align}
%    This can be equivalently written as
%    \begin{align}
%        I_{(n,k)}(f)(\theta) &= \frac{((k+1)n + 1)\sigma_{(k+1)n + 1}(f)(\theta) - (kn + 1)\sigma_{kn + 1}(f)(\theta)}{n} \\
%        &= \left( k+1 + \frac{1}{n} \right)\sigma_{(k+1)n + 1}(f)(\theta) - \left( k + \frac{1}{n} \right)\sigma_{kn + 1}(f)(\theta) \label{eq:4}
%    \end{align}
    Since we know that $\sigma_n(f)$ converges uniformly to $f$, expression \ref{eq:4} will also converge to $f$ as $n$ goes to $\infty$, for all $k$, although the rate of convergence will depend on $k$.
    
    Now consider another way of writing $I_{(n,k)}(f)(\theta)$ (one needs to write out all the individual terms to see this)
    \begin{align*}
        I_{(n,k)}(f)(\theta) = \sum_{q=-kn}^{kn} \widehat{f}(q) e^{iq\theta} + \sum_{kn < |q| \leq (k+1)n} \frac{(k+1)n + 1 - |q|}{n} \widehat{f}(q)(n) e^{iq\theta}
    \end{align*}
    Since $\widehat{f}(k)$ is $O\left( \frac{1}{k} \right)$, it is bounded above by $\frac{A}{k}$ for some constant $A$.
    
    Writing $S_m(f)(\theta)$ similarly
    \begin{align*}
     S_m(f)(\theta) = \sum_{q=-kn}^{kn} \widehat{f}(q) e^{iq\theta} + \sum_{kn < |q| \leq m} \widehat{f}(q)(n) e^{iq\theta}
    \end{align*}
    where $kn \leq m < (k+1)n$.
    Taking their difference, we get
    \begin{align*}
        |I_{(n,k)}(f)(\theta) - S_m(f)(\theta)| &\leq \sum_{kn < |q| \leq (k+1)n} 2|\widehat{f}(q)| \\
        &\leq 4n \cdot \frac{A}{kn} \\
        &= \frac{4A}{k}
    \end{align*}
    
    For $\vep > 0$, pick a $k$ such that $\frac{2A}{k} < \frac{\vep}{2}$. Pick an $N > k$ such that for all $n > N$,
    \begin{align}
        \norm{I_{(n,k)}(f) - f}_{\infty} < \frac{\vep}{2} \label{ineq:1}
    \end{align}
    Now pick any $m > kN$. Since $N > k$, we have the following inequality
    \begin{align*}
        k(n+1) < (k+1)n
    \end{align*}
    where $n > N$. That means for every $m > kN$, there exists $n > N$, such that
    \begin{align*}
        kn \leq  m < (k+1)n
    \end{align*}
    That means for all $m > kN$
    \begin{align}
        \norm{S_m(f) - I_{(n,k)}(f)}_\infty < \frac{\vep}{2} \label{ineq:2}
    \end{align}
    Combining inequalities \ref{ineq:1} and \ref{ineq:2}, we get
    \begin{align*}
        \norm{S_m(f) - f}_\infty < \vep
    \end{align*}
    This shows that the partial Fourier sums converge uniformly the function $f$.
\end{proof}

\subsection{A continuous function whose Fourier series diverges at a point \cite{banach}}
The last section showed that the Fourier series of a $C^1$ function does converge; this section aims to show that that's almost the best result one can get, i.e. there does exist a continuous function whose Fourier series not only does not converge to the function uniformly, it does not even converge pointwise.

Consider the sequence of kernels described in equation \ref{eq:5}
\begin{align*}
    g_N(t) = \frac{1}{2\pi} \frac{\sin\left( Nt + \frac{t}{2} \right)}{\sin\left( \frac{t}{2} \right)}
\end{align*}
where $(f \ast g_N)$ corresponds to the $N$\textsuperscript{th} partial Fourier sum of $f$. The first thing we will do is bound the $L^1$ norm of $g_N$ from below.
\begin{align*}
    \norm{g_N}_1 &= \frac{1}{2\pi} \int_{-\pi}^{\pi} \left| \frac{\sin \left( Nt + \frac{t}{2} \right)}{\sin \left( \frac{t}{2} \right)} \right| dt \\
    &\geq \frac{1}{\pi} \int_{-\pi}^{\pi}\left| \frac{\sin \left( Nt + \frac{t}{2} \right)}{t} \right|dt &&\text{($|x| \geq |\sin(x)|$)} \\
    &= \frac{1}{\pi} \int_{-\pi\left( N+\frac{1}{2} \right)}^{\pi\left( N+\frac{1}{2} \right)} \left| \frac{\sin(y)}{y} \right| dy \\
%    &> \frac{1}{\pi} \int_{-\pi\left( N+\frac{1}{2} \right)}^{-\frac{\pi}{2}} \left| \frac{\sin(y)}{y} \right| dy \\
    &> \frac{1}{\pi} \sum_{k=1}^{\frac{N}{2}} \int_{-\pi\left( k +1 \right)}^{-\pi\left( k - 1 \right)} \left| \frac{\sin(y)}{y} \right| dy \\
    &\geq \frac{1}{\pi} \sum_{k=1}^{\frac{N}{2}} \frac{\int_{-\pi}^{\pi} |\sin(x)|}{-\pi\left( k + \frac{1}{2} \right)} \\
    &\geq k \log(N) &&\text{(For some $k>0$)}
\end{align*}

Now consider the function $q_N(x) = \mathrm{sign}(g_N(-x))$. In that case
\begin{align*}
    S_N(q_N)(0) = \norm{g_N}_1
\end{align*}
And we can approximate $q_N$ with a continuous function whose absolute value is bounded by $1$ as closely as possible; this gives us a sequence of functions $j_N$ such that
\begin{align*}
    s_N(j_N)(0) &\geq \norm{g_N}_1 - 1 \\
    \norm{j_N}_\infty &\leq 1
\end{align*}

But $S_N(\cdot)(0)$ is a continuous operator from the Banach space $C(T)$ with the uniform norm to the space $\mathbb{C}$. Since $\norm{g_N}_1$ is unbounded, that means the operator norm of $S_N(\cdot)(0)$ is also not uniformly bounded. But from the uniform boundedness principle, if a continuous operator from a Banach space is not uniformly bounded, it's not pointwise bounded either for some point $x$ in the domain. This means the sequence $\norm{S_N(f)(0)}$ is not bounded for some $f \in C(T)$. For that continuous function $f$, the Fourier series diverges.
\newpage

\section{Weyl's equidistribution theorem}
In the previous section, we saw how the idea of Fourier series emerged from a physical problem; the rest of the section was devoted to establishing firm foundations for the notion of Fourier series, especially those of convergence. With the foundational work done, one can look at the applications of the theory. In this and the subsequent section, we'll look at two theorems that use Fourier analysis; the surprising thing is that both these theorems are in number theory, quite distant from physics, where Fourier analysis first emerged. The fact that one can use tools that emerged in the analysis of a physical system for solving problems in number theory is a testament to the power and versatility of the tool.

In this section, we'll look at Weyl's equidistribution theorem, which informally states that a sequence of multiples of an irrational number modulo $1$ is equidistributed in the interval $[0,1]$.

\subsection{Equidistributed sequences}
We'll begin by formally defining what equidistributed sequences are.
\begin{defn}
    A sequence $\{q_n\}$ is said to be equidistributed in the interval $[a,b]$ if
    \begin{itemize}
        \item $q_n \in [a,b]$ for all $n \in \mathbb{N}$.
        \item For all subintervals $[c,d]$ of $[a,b]$
                \begin{align*}
                    \lim_{N \to \infty} \frac{1}{N} \sum_{k=1}^{N} \indi_{[c,d]}(q_k) = \frac{d-c}{b-a}
                \end{align*}
                where $\indi_{[c,d]}$ is the indicator function for the interval $[c,d]$.
    \end{itemize}
\end{defn}
For now, let's just focus on the interval $[0,2\pi]$, and look for equidistributed sequences in it. Observe that the second part of the above definition can be rewritten as
\begin{align}
    \lim\limits_{N \to \infty} \frac{1}{N} \sum_{k=1}^{N} \indi_{[c,d]}(q_k) = \frac{1}{2\pi} \int_{0}^{2\pi} \indi_{[c,d]}(x) dx \label{eq:6}
\end{align}

\subsection{Weyl's criterion for equidistribution}
The above definition for equidistributed sequences makes sense (in the manner that agrees with our intuition), it is a little cumbersome to work with, since one needs to sum the indicator function of every subinterval of $[0,2\pi]$ over the entire sequence to ensure it really is equidistributed. It would be nicer if one had a simpler, but equivalent notion for equidistribution that was easier to work with. It turns out there is one.

\begin{lem} \label{lem:1}
    A subsequence $\{q_k\}$ of $[0,2\pi]$ is equidistributed iff for every continuous function $f$ on $[0,2\pi]$
    \begin{align*}
        \lim\limits_{N \to \infty} \frac{1}{N} \sum_{k=1}^{N} f(q_k) = \frac{1}{2\pi} \int_{0}^{2\pi} f(x)dx
    \end{align*}
\end{lem}

\begin{proof}
    First we'll show that if the sequence is equidistributed then the above equation is correct. Consider \emph{simple functions}\footnote{Simple functions are functions whose range is finite. These functions can be written as linear combinations of indicator functions.} on $[0,2\pi]$. Since these functions are linear combinations of indicator functions, and the indicator functions satisfy equation \ref{eq:6}, the simple functions also satisfy that equation, i.e.
    \begin{align*}
        \lim\limits_{N \to \infty} \frac{1}{N} \sum_{k=1}^{N} s(q_k) = \frac{1}{2\pi} \int_{0}^{2\pi} s(x)dx
    \end{align*}
    where $s$ is a simple function. Also, note the fact that in the compact interval $[0, 2\pi]$, for every $\vep > 0$ and for every continuous function $f$, there exists a simple function $s$, such that $\norm{f - s}_\infty < \vep$.
    
    Now we'll show that for every $\vep >0$, and for every continuous function $f$, there exists an $N \in \mathbb{N}$ such that for all $n > N$
    \begin{align*}
        \left| \frac{1}{n} \sum_{k=1}^{n} f(q_k) - \frac{1}{2\pi} \int_{0}^{2\pi} f(x)dx \right| < \vep
    \end{align*}
    Pick a simple function $s$ such that $\norm{f-s}_\infty < \frac{\vep}{3}$. We'll have the following inequalities
    \begin{align}
        \frac{1}{2\pi}\left| \int_{0}^{2\pi} f(x)dx - \int_{0}^{2\pi} s(x)dx \right| &\leq \frac{1}{2\pi} \left| \int_{0}^{2\pi} (f(x) -s(x))dx \right| \\
        &\leq \frac{\vep}{3} \label{ineq:3}
    \end{align}
    And for all $n \in \mathbb{N}$
    \begin{align}
        \left| \frac{1}{n} \sum_{k=1}^{n} f(q_k) - \frac{1}{n} \sum_{k=1}^{n} s(q_k) \right| &\leq \left| \frac{1}{n} \sum_{k=1}^{n} f(q_k) -s(q_k) \right| \\
        &\leq \frac{\vep}{3} \label{ineq:4}
    \end{align}
    Now pick an $N$ large enough such that for all $n > N$
    \begin{align}
        \left| \frac{1}{n} \sum_{k=1}^{n} s(q_k) - \frac{1}{2\pi} \int_{0}^{2\pi} s(x)dx \right| < \frac{\vep}{3} \label{ineq:5}
    \end{align}
    Adding up inequalities \ref{ineq:3}, \ref{ineq:4}, and \ref{ineq:5}, we have the inequality we wanted.
    
    Now for the converse statement. The key idea here is to approximate the indicator function $\indi_{[c,d]}$ with an appropriate continuous function. For a given $\vep > 0$, consider the following continuous approximation for the function $\indi_{[c,d]}$
    \begin{align*}
        f(x) =
        \begin{cases} 
        \hfill 1    \hfill & \text{$x \in (c,d)$ } \\
        \hfill \frac{8x - 8c + \vep}{\vep} \hfill & \text{ $x \in \left[c-\frac{\vep}{8}, c \right]$ } \\
        \hfill \frac{8d - 8x + \vep}{\vep} \hfill & \text{ $x \in \left[d, d+\frac{\vep}{8} \right]$ } \\
        \hfill 0 \hfill & \text{otherwise}
        \end{cases}
    \end{align*}
    For all $x \in [0,2\pi]$, we have the following inequality for the function $f$
    \begin{align*}
        \indi_{[c,d]}(x) \leq f(x) \leq \indi_{\left[ c- \frac{\vep}{8}, d+\frac{\vep}{8} \right]}(x) 
    \end{align*}
    Rewriting the above inequality,
    \begin{align*}
        \left| f(x) - \indi_{[c,d]}(x) \right| \leq \indi_{\left[ c-\frac{\vep}{8}, c \right]}(x) + \indi_{\left[ d, d+\frac{\vep}{8} \right]}(x)
    \end{align*}
    This means there exists a large enough $N_1$, such that for all $n > N_1$
    \begin{align*}
        \frac{1}{n} \sum_{k=1}^{n} \left( \indi_{\left[ c-\frac{\vep}{8}, c \right]}(q_k) + \indi_{\left[ d, d+\frac{\vep}{8} \right]}(q_k) \right) \leq \frac{\vep}{3}
    \end{align*}
    For that same $N_1$,
    \begin{align}
        \left| \frac{1}{n}\sum_{k=1}^{n}\indi_{[c,d]}(q_k) - \frac{1}{n}\sum_{k=1}^{n}f(q_k) \right| \leq \frac{\vep}{3} \label{ineq:6}
    \end{align}
    Similarly,
    \begin{align}
        \frac{1}{2\pi} \left| \int_{0}^{2\pi} f(x) dx - \int_{0}^{2\pi}\indi_{[c,d]}(x)dx \right| \leq \frac{\vep}{3} \label{ineq:7}
    \end{align}
    Finally, because of our hypothesis, we have a large enough $N_2$ such that for all $n > N_2$,
    \begin{align}
        \left| \frac{1}{n}\sum_{k=1}^{n}f(q_k) - \frac{1}{2\pi}\int_{0}^{2\pi}f(x)dx \right| \leq \frac{\vep}{3} \label{ineq:8}
    \end{align}
    Let $N = \max(N_1, N_2)$, and adding up inequalities \ref{ineq:6}, \ref{ineq:7}, and \ref{ineq:8}, we get our result.
    
    This completes the proof of equivalence.
\end{proof}

This result shows how to equidistribution in terms of continuous functions. The next result will use Fejér's theorem to break up the result about continuous functions to exponential sums, giving us a much simpler formalism to work with.

\begin{thm}
    A subsequence $\{q_k\}$ of $[0,2\pi]$ is equidistributed iff for all non-zero integers $a$
    \begin{align*}
        \lim\limits_{n \to \infty} \frac{1}{n} \sum_{k=1}^{n} \exp(iaq_k) = 0
    \end{align*}
\end{thm}

\begin{proof}
    First we'll show that if the sequence is equidistributed, then the limit is $0$. Since $\exp(iax)$ is a continuous function for all $a$, from lemma \ref{lem:1}, we have
    \begin{align*}
        \lim\limits_{n \to \infty} \frac{1}{n} \sum_{k=1}^{n} \exp(iaq_k) &= \frac{1}{2\pi} \int_{0}^{2\pi} \exp(iax)dx \\
        &= 0 &&\text{($a$ is non-zero)}
    \end{align*}
    
    For the converse, we will use Fejér's theorem. Given a continuous $f$ and $\vep > 0$, use Fejér's theorem to find an exponential polynomial $p$ such that $\norm{p-f}_\infty < \frac{\vep}{3}$. In that case, we have
    \begin{align}
        \frac{1}{2\pi}\left| \int_{0}^{2\pi} f(x) dx - \int_{0}^{2\pi} p(x) dx \right| < \frac{\vep}{3}  \label{ineq:9}
    \end{align}
    and
    \begin{align}
        \left| \frac{1}{n} \sum_{k=1}^{n} f(q_k) - \frac{1}{n} \sum_{k=1}^{n} p(q_k) \right| < \frac{\vep}{3} \label{ineq:10}
    \end{align}
    And because of our hypothesis, we have a large enough $N$, such that for all $n > N$
    \begin{align}
        \left| \frac{1}{n} \sum_{k=1}^{n} p(q_k) -  \int_{0}^{2\pi} p(x) dx \right| \leq \frac{\vep}{3} \label{ineq:11}
    \end{align}
    Adding up inequalities \ref{ineq:9}, \ref{ineq:10}, and $\ref{ineq:11}$, we get our result. 
\end{proof}

\subsection{Weyl's equidistribution theorem: The weak version}
The weak version of Weyl's equidistribution theorem states that the sequence
\begin{align*}
    q_k = kz \mod 2\pi
\end{align*}
is equidistributed in the interval $[0,2\pi]$ if $z$ is an irrational multiple of $\pi$. This can be shown really easily using the earlier formalisation, by looking at the exponential sums.
\begin{align*}
    \sum_{k=1}^{n} \exp(iakz) &= e^{iaz} \frac{1 - e^{ianz}}{1 - e^{iaz}}
\end{align*}
Since $z$ is an irrational multiple of $\pi$, the denominator is non-zero for all non-zero integers $a$. That means for a given $a$, the sum is bounded, hence
\begin{align*}
    \lim\limits_{n \to \infty} \frac{1}{n} \sum_{k=1}^{n} \exp(iakz) = 0
\end{align*}
and as a consequence, the sequence is equidistributed.

\subsection{Weyl's equidistribution theorem: The stronger version}
\textbf{Note:} Since it will get a little cumbersome to keep working with integral and irrational multiples of $2\pi$, we will instead deal with sequences in the interval $[0,1]$. The only change we'll need to keep in mind is that we'll be dealing the following sum
\begin{align*}
     \sum_{k=1}^{n} \exp(2\pi iaq_k)
\end{align*}

The stronger version of Weyl's equidistribution theorem states that the sequence
\begin{align*}
    q_k = \alpha k^2 + \beta k + \gamma \mod 1
\end{align*}
is equidistributed if $\alpha$ is irrational.
 
This will involve showing that the sequence
\begin{align}
    S_n = \sum_{k=1}^{n} \exp(2\pi i aq_k) \label{target:1}
\end{align}
is $o(n)$ for all integers $n$, but since $\alpha$ is irrational, it suffices to show it for all irrational $\alpha$. In a nutshell, we have reduced the problem of equidistribution to a problem of bounding the given sum.
 
\subsubsection{A preliminary bound for another sum}
\textbf{Notation:} We will define the function $e(x)$ to be $e(2\pi i x)$.

\begin{lem}
    For all irrational $\theta$
    \begin{align}
    \displaystyle \left| \sum_{k=1}^{N} e(n\theta) \right| \leq \min\left(N, \frac{1}{\norm{\theta}} \right) \label{bound:1}
    \end{align}
    where $\norm{\theta}$ is distance of $\theta$ to its closest integer.
\end{lem}

\begin{proof}
    By triangle inequality,
    \begin{align*}
        \left| \sum_{k=1}^{N} e(n\theta) \right| & \leq \sum_{k=1}^{N} \left| e(n\theta) \right| \\
        &= N
    \end{align*}
    
    And summing up the geometric series, we have
    \begin{align*}
        \left| \sum_{k=1}^{N} e(n\theta) \right| &\leq \frac{2}{\left| 1- e(\theta) \right|} \\
        &= \frac{1}{\left| \sin(\pi \theta) \right|}\\
        &\leq \frac{1}{\norm{\theta}}
    \end{align*}
\end{proof}
 
\subsubsection{Weyl differencing} \label{sect:1}
Since we want to show the sum $S_N$ (see equation \ref{target:1}) is in $o(N)$, it will suffice to show that $\left| S_N \right|^2$ is in $o(N^2)$.
\begin{align*}
    \left| \sum_{k=1}^{N} e(f(k)) \right|^2 &= \left( \sum_{k=1}^{N} e(f(k)) \right) \left( \sum_{k=1}^{N} e(-f(k)) \right) \\
    &= \sum_{1 \leq j,l \leq N} e(f(j) - f(l)) \\
    &= N + \sum_{d=1}^{N-1} \sum_{k=1}^{N-d} \left( e(f(k+d) - f(k)) + e(f(k) - f(k+d)) \right) \\
    &= N + \sum_{d=1}^{N-1} \sum_{k=1}^{N-d} 2\Re \left( e(f(k+d) - f(k)) \right) \\
    &= N + 2 \sum_{d=1}^{N-1} \Re \left( \sum_{k=1}^{N-d} e(f(k+d) - f(k)) \right) \\
    &\leq N + 2 \sum_{d=1}^{N-1} \left| \sum_{k=1}^{N-d} e(f(k+d) - f(k)) \right|
\end{align*}
Plugging in the quadratic polynomial in $f$, we get
\begin{align*}
    \left| \sum_{k=1}^{N} e(q_k) \right|^2 &\leq N + 2\sum_{d=1}^{N-1} \left| \sum_{k=1}^{N-d} e((2\alpha d)k) \right| \\
    &\leq N + 2 \sum_{d=1}^{N} \min \left( N, \frac{1}{\norm{2\alpha d}} \right)
\end{align*}
Since $N$ is in $o(N^2)$, all we need to show is that $\displaystyle \sum_{d=1}^{N} \min\left( N, \frac{1}{\norm{2\alpha d}} \right)$ is also in $o(N^2)$.

\subsubsection{Rational approximation of irrational numbers}
The main result of this section will show that given an irrational number $\alpha$, it's possible to find infinitely many rational numbers $\frac{p}{q}$ ($\gcd(p,q) = 1$) such that $|\alpha - \frac{p}{q}| < \frac{1}{q^2}$. We will need this result to bound the main sum.

\begin{prop} \label{prop:1}
    Given an irrational number $\alpha$ and $N \in \mathbb{N}$, there exists a rational number $\frac{p}{q}$ such that
    \begin{align*}
        \left| \alpha - \frac{p}{q} \right| < \frac{1}{q(N+1)}
    \end{align*}
    and $|q| \leq N$.
\end{prop}

\begin{proof}
    Consider the following sequence modulo $1$:
    \begin{align*}
        \alpha, 2\alpha, 3\alpha, \ldots, N\alpha
    \end{align*}
    Divide $[0,1]$ into $N+1$ equally sized intervals. Clearly, one of the elements of that sequence must either lie in the subinterval $\left[ 0, \frac{1}{N+1} \right]$, or $\left[ \frac{N}{N+1}, 1 \right]$. Call that element $q\alpha$. In that case, the following inequality holds:
    \begin{align*}
        \norm{q\alpha} \leq \frac{1}{N+1}
    \end{align*}
    Let the integer closest to $q\alpha$ be $p$. In that case
    \begin{align*}
        \left| p - q\alpha \right| &\leq \frac{1}{N+1} \\
        \left| \frac{p}{q} - \alpha \right| &\leq \frac{1}{q(N+1)}
    \end{align*}
    And since $q < N+1$
    \begin{align*}
        \left| \alpha - \frac{p}{q} \right| < \frac{1}{q^2}
    \end{align*}
\end{proof}

\begin{cor}
    If $\alpha$ is irrational, there are infinitely many rational numbers $\frac{p}{q}$ such that
    \begin{align*}
        \left| \alpha - \frac{p}{q} \right| < \frac{1}{q^2}
    \end{align*}
\end{cor}

\begin{proof}
    Suppose there were finitely many. Take the minimal distance $d$ of $\alpha$ with the rational approximations. Pick a large enough $N$ such that $\frac{1}{N+1} < d$. Use that $N$ in proposition \ref{prop:1} and get a contradiction.
\end{proof}

\subsubsection{Bounding the complete sum\cite{weyl}}
At the end of section \ref{sect:1}, we saw that all we need to do is to show that the following sum
\begin{align*}
    W_N = \sum_{d=1}^{N} \min\left( N, \frac{1}{\norm{2\alpha d}} \right)
\end{align*}
is in $o(N^2)$.

\begin{lem} \label{lem:2}
    If $\alpha_1, \alpha_2, \ldots, \alpha_N$ are real numbers such that $\norm{\alpha_i - \alpha_j} \geq \frac{1}{r}$ for some natural number $r$ when $i \neq j$. Then
    \begin{align*}
        \sum_{i=1}^{N} \min\left( \frac{1}{\norm{\alpha_i}}, N \right) \leq 2N + 2r(\log(N) + 2)
    \end{align*} 
\end{lem}

\begin{proof}
    Without loss of generality, assume all the $\alpha_i$ lie within $\left[ -\frac{1}{2}, \frac{1}{2} \right]$. Furthermore, at least half the sum will either come from the positive or the negative $\alpha_i$. WLOG, assume it's the positive $\alpha_i$. Then we can just double the sum to get an upper bound. Assume that $\alpha_1 < \alpha_2 < \cdots < \alpha_n$, where $\alpha_1$ to $\alpha_n$ are the positive $\alpha_i$. In that case
    \begin{align*}
        \sum_{i=1}^{n}\min\left( \frac{1}{\norm{\alpha_i}}, N  \right) \leq N+ \left\lfloor \frac{r}{N} \right\rfloor N + \sum_{i > \left\lfloor \frac{r}{N} \right\rfloor} \frac{r}{i} &&\left(\text{Because $\norm{\alpha_i -\alpha_j} > \frac{1}{r}$}\right)
    \end{align*}
    The last term can have at most $n$ terms, hence it is bounded by $r (\log(n) + 1)$. But $n$ is smaller than or equal to $N$, since $n$ is the number of positive $\alpha_i$. Hence the last term is bounded by $r(\log(N) + 1)$. The second term is bounded by $r$, and the first term is $N$. Hence the complete bound is
    \begin{align*}
        \sum_{i=1}^{n}\min\left( \frac{1}{\norm{\alpha_i}}, N  \right) \leq N + r + r(\log(n) + 1)
    \end{align*}
    Since this is at least half the total sum, we get our bound.
\end{proof}

\begin{lem}
    If $\frac{p}{q}$ is a rational approximation for $\alpha$ such that $\left| \alpha - \frac{p}{q} \right| < \frac{1}{q^2}$, then
    \begin{align*}
        \sum_{d=1}^{N} \min\left( \frac{1}{\norm{\alpha d}}, N \right) \leq \left( \frac{2N}{q}\right) \left( 2N +4q(\log(N) +2) \right)
    \end{align*}
\end{lem}

\begin{proof}
    We have the following inequality, which is not too hard to prove:
    \begin{align*}
        \norm{i\alpha - j\alpha} \geq \norm{(i-j)\frac{p}{q}} - \frac{|i-j|}{q^2} &&\left(\text{Taylor expand around $\frac{p}{q}$}\right)
    \end{align*}
    When $|i-j| \leq \frac{q}{2}$, then $\frac{(i-j)p}{q}$ is not an integer since $p$ and $q$ are co-prime. That means
    \begin{align*}
        \norm{(i-j)\frac{p}{q}} \geq \frac{1}{q}
    \end{align*}
    and 
    \begin{align*}
        \frac{|i-j|}{q^2} \leq \frac{1}{2q}
    \end{align*}
    which means
    \begin{align*}
        \norm{i\alpha - j\alpha} \geq \frac{1}{2q}
    \end{align*}
    Now we split up the range of summation, i.e. the interval $[1, N]$ into intervals of length $\frac{q}{2}$. To compute the sum over each of these intervals, we'll use lemma \ref{lem:2} with $r = 2q$. We'll have $\frac{2N}{q}$ such intervals, over each of which, the sum will be be bounded by $\left( 2N +4q(\log(N) +2) \right)$. The complete bound is hence
    \begin{align*}
        W_N \leq \left( \frac{2N}{q}\right) \left( 2N +4q(\log(N) +2) \right)
    \end{align*}
\end{proof}
As $N$ goes to infinity, the $\frac{W_N}{N^2}$ becomes $\frac{2}{q}$. But since $\alpha$ is irrational, $q$ can be as large as possible. This shows that $W_N$ is in $o(N^2)$.

\newpage

\section{Roth's theorem}
The fact that the integral
\begin{align*}
    \int_{-\pi}^{\pi} e^{inx} dx
\end{align*}
is $1$ only when $n$ is $0$ and for other integral values of $n$, it is $0$, can be very useful. It is this very observation that forms the backbone of a general class of techniques in number theory called the \emph{circle method}. In a nutshell, this method lets one \emph{count}, by integrating an appropriately picked function over the unit circle, hence the name.

Roth's theorem is question about existence of three term APs, but like many existence problems, it can be resolved by counting, and making sure your count doesn't end at $0$. Formally, the theorem states that given a number $\delta > 0$, called the density, there exists a large enough $N$ such that any subset of $\{0,1, 2, 3, \ldots, N-1\}$ with a size greater than $\delta N$ contains a  three term arithmetic progression.  The key idea in the proof is to count the number of triples $x,y,z$ such that $x+z-2y$ is $0$. And this is where the circle method comes in, transforming a combinatorial problem into a problem in analysis.

\subsection{Sketch of proof}
The key idea in the proof of Roth's theorem is that for a given $0 < \delta < 1$, and a large enough $N$, if a subset $A$ of $[N]$\footnote{$[N]$ is shorthand for the set $\{0,1, 2,3 \ldots, N-1\}$}, which has size $\delta N$ does not have a three term AP, then there exists a progression $B_1$ in $[N]$ such that the density of $A$ in $B_1$ is greater than $\delta$. Clearly, $A \cap B_1$ won't contain a three term AP either, so one iterates the argument again and again to get a sequence of nested sub progressions $B_1, B_2, \ldots B_k$ such that the density of $A$ in $B_k$ is at least $1$. But that would mean $B_k \subset A$, and if $|B_k| \geq 3$, we'll have a contradiction. So one picks a large enough $N$ such that $|B_k| \geq 3$, and that leads to the contradiction. Which means $A$ must have had a three term AP.

\subsection{Fourier analysis on finite cyclic groups}
Let's begin by endowing the set $[N]$ some additional structure, namely identifying it with the group $\znz$. Furthermore, with the counting measure $\znz$, we can integrate functions from $\znz$ to $\mathbb{C}$. And with the discrete metric, we have the topology on the space; we can finally do some analysis.

Consider the set of functions from $\znz$ to $\mathbb{C}$. This clearly forms a Hilbert space, with the inner product being
\begin{align*}
   \iprod{f}{g} &= \frac{1}{N} \int_{\znz} f(x) \overline{g(x)} dx \\
   &= \frac{1}{N} \sum_{k=0}^{N-1} f(k) \overline{g(k)}
\end{align*}
The next step would be to determine an orthonormal basis for the Hilbert space. It turns out that the set of homomorphisms from $\znz$ to $\mathbb{C}$ does form an orthonormal basis for the space. Define $h_r$ to be the following homomorphism
\begin{align*}
    h_r(k) = e^{\frac{2\pi i}{N}rk}
\end{align*}
Notice that
\begin{align*}
    \frac{1}{N}\int_{\znz} h_r(x) dx =
    \begin{cases}
        1 \hfill &\text{if $r=0$} \\
        0 \hfill &\text{if $r \neq 0$}
    \end{cases}
\end{align*}
The $r$\textsuperscript{th} Fourier coefficient of $f$ is defined as
\begin{align*}
    \widehat{f}(r) &= \iprod{f}{h_r}\\ 
    &= \frac{1}{N} \sum_{k=0}^{N-1} f(k) e^{-\frac{2\pi i}{N}rk} 
\end{align*}
Hence, for any function $f$
\begin{align*}
    f(x) = \widehat{f}(0) h_0(x) + \widehat{f}(1) h_1(x) + \ldots + \widehat{f}(N-1) h_{N-1}(x)
\end{align*}

\subsection{Counting progressions in $[N]$\cite{toronto}}
\textbf{Note:} We'll call $(x,y,z)$ a progression in $\znz$ when $x+z=2y$ in the group $\znz$, and we'll call it a progression in $[N]$ when $x+y=2z$ in $\mathbb{Z}$.

Consider a subset $A$ of $[N]$ of size $\delta N$. Identify $A$ as a subset of $\znz$ as well. We are looking for three term APs, or triples $(x,y,z)$ such that $x+z - 2y = 0$ in $\znz$. The number of such triples will be given by
\begin{align*}
    S_0 &= \sum_{x,y,z \in A} \frac{1}{N} \int_{\znz} h_{x+z-2y}(x) dx \\
    &= \sum_{x,y,z \in [N]} \frac{1}{N} \sum_{k=0}^{N-1} \indi_{A}(x) \indi_{A}(y) \indi_{A}(z) e^{-\frac{2\pi i}{N} k (x+z-2y)} \\
    &= N^2 \sum_{k=0}^{N-1} \widehat{\indi_A}(k)^2 \widehat{\indi_A}(-2k)
\end{align*}
where $\indi_A$ is the indicator function of $A$. This counts the number of three term APs in the group $\znz$. But all the progressions in $\znz$ need not be progressions in $[N]$. But if we know that $x$ and $y$ belong to $\left[ \frac{N}{3}, \frac{2N}{3} \right]$, and $x+z-2y=0$ in the group, then $x,y,z$ form an AP in $[N]$. Define the set $M_A$ to be $\left[ \frac{N}{3}, \frac{2N}{3} \right] \cap A$. Then, a lower bound for the number of three term APs in $[N]$ is given by
\begin{align*}
    S &= \sum_{x,y \in M_A} \sum_{z \in A}  \frac{1}{N} \int_{\znz} h_{x+z-2y}(x) dx \\
    &= N^2 \sum_{k=0}^{N-1} \widehat{\indi_A}(k) \widehat{\indi_{M_A}}(k) \widehat{\indi_{M_A}}(-2k) \\
    &= N^2 \left(\widehat{\indi_A}(0) \widehat{\indi_{M_A}}(0) \widehat{\indi_{M_A}}(0) + \sum_{k=1}^{N-1} \widehat{\indi_A}(k) \widehat{\indi_{M_A}}(k) \widehat{\indi_{M_A}}(-2k) \right) \\
    &= \delta |M_A|^2 + N^2 \sum_{k=1}^{N-1} \widehat{\indi_A}(k) \widehat{\indi_{M_A}}(k) \widehat{\indi_{M_A}}(-2k)
\end{align*}
We want to show that $[N]$ contains at least one three term $AP$. But the expression for $S$ also counts triples like $(x,x,x)$, which we don't want to count as APs. $A$ contains $|A|$ such triples. Hence we'd like $S$ to be greater than $|A|$ to show the existence of an AP. We have the following lemma
\begin{lem}\label{lem:3}
    If $N$ is odd, $\widehat{\indi_A}(k) < \vep$ for all $k \neq 0$, where $\vep = \frac{\delta^2}{8}$, and $|M_A| \geq \frac{\delta N}{4}$, then $S \geq \frac{\delta^3 N^2}{32}$.
\end{lem}

\begin{proof}
    We know that
    \begin{align*}
        S = \delta |M_A|^2 + N^2 \sum_{k=1}^{N-1} \widehat{\indi_A}(k) \widehat{\indi_{M_A}}(k) \widehat{\indi_{M_A}}(-2k)
    \end{align*}
    We can bound the second term in the sum using Cauchy-Schwarz inequality and Plancherel's identity.
    \begin{align*}
        \left| \sum_{k=1}^{N-1} \widehat{\indi_A}(k) \widehat{\indi_{M_A}}(k) \widehat{\indi_{M_A}}(-2k) \right|
        &\leq \vep \sum_{k=1}^{N-1}\left| \widehat{\indi_{M_A}}(k) \widehat{\indi_{M_A}}(-2k) \right| \\
        &\leq \vep \left( \sum_{k=1}^{N-1} \left| \widehat{\indi_A}(k) \right|^2 \right)^{\frac{1}{2}} \left( \sum_{k=1}^{N-1} \left| \widehat{\indi_A}(-2k) \right|^2 \right)^{\frac{1}{2}} \\
        &= \vep \left( \sum_{k=1}^{N-1} \left| \widehat{\indi_A}(k) \right|^2 \right) \\
        &= \vep \left( \sum_{k=1}^{N-1} \left| \indi_A(k) \right|^2 \right) \\
        &= \vep |M_A|
    \end{align*}
    From this, we get that
    \begin{align*}
        S \geq \delta |M_A|^2 - \vep N^2 |M_A|
    \end{align*}
    Since $|M_A| \geq \frac{\delta N}{4}$, and $\vep = \frac{\delta^2}{8}$, we get
    \begin{align*}
        S \geq \frac{\delta^3 N^2}{32}
    \end{align*}
\end{proof}
This lemma shows that if we pick an $N > \frac{32}{\delta^2}$, any subset $A$ which satisfies the hypotheses of the lemma will contain a $3$-AP.

\subsection{Density incrementation}
Now consider the contrapositive of lemma \ref{lem:3}. It says that if a subset $A$ of $[N]$ does not contain a $3$-AP, then one of the following conditions must hold
\begin{enumerate}
    \item For some non-zero $k$, $\widehat{\indi_A}(k) > \vep$, where $\vep = \frac{\delta^2}{8}$.
    \item $|M_A| < \frac{\delta N}{4}$.
\end{enumerate}
We don't consider the third condition where $N$ could be less than $\frac{32}{\delta^2}$ because we can make $N$ as large as we want. Consider the second condition. If the density of $A$ in $\left[ \frac{N}{3}, \frac{2N}{3} \right]$ is less than $\frac{\delta N}{4}$, then by the pigeonhole principle, $A$ has a density greater than $\delta + \frac{\delta}{8}$ in either $\left[ 0, \frac{N}{3} \right]$ or $\left[ \frac{2N}{3}, N \right]$. To put it more generally, there exists an AP $Z$ in $[N]$ of length greater than or equal to $\frac{N}{3}$ such that the density of $A$ in $Z$ is $\delta + \frac{\delta}{8}$, which is greater than the original density. Specifically, the progression $Z$ in this case is either the interval $\left[ 0, \frac{N}{3} \right]$ or $\left[ \frac{2N}{3}, N \right]$.

We will show even when $\widehat{\indi_A}(k) > \vep$ for some non-zero $k$, there exists a sufficiently long progression in $[N]$ such that the density of $A$ in that subprogression is greater than $\delta$.

\begin{lem} \label{dens:1}
    If $\widehat{\indi_A}(r) > \gamma$ for some $r \neq 0$, then there exists a $\znz$ subprogression $B$ of length at least $\frac{\sqrt{N}}{8}$ such that $|A \cap B| \geq \left( \delta + \frac{\gamma}{4} \right)|B| $.
\end{lem}

\begin{proof}
    Consider the pairs of points
    \begin{align*}
        (0, 0), (1, r), (2, 2r), \ldots, (N-1, (N-1)r)
    \end{align*}
    These points lie in the square $[0, N] \times [0, N]$. Divide this square into $\lfloor \sqrt{N} \rfloor^2$ squares (where $\lfloor x \rfloor$ is the greatest integer \emph{less} than $x$) of side length $l = \frac{N}{\lfloor \sqrt{N} \rfloor}$. Since there are less than $N$ squares, two of the points must lie in the same square. That means for some $d \leq l$
    \begin{align*}
        rd \leq l\ (\text{mod } N)
    \end{align*}
    Let $B'$ be an AP of length $ \left\lfloor \frac{\lfloor N \rfloor}{2\pi} \right\rfloor$ in $\znz$
    \begin{align*}
        \ldots , -3d, -2d, -d, 0, d, 2d, 3d, \ldots
    \end{align*}
    Since $|B'|d$ is less than $|B'|l$, which in turn is less than $\frac{N}{2\pi}$, which means that it can be written as a union of two disjoint APs in $[N]$.

    Consider the following inequality:
    \begin{align*}
        \left|N \widehat{\indi}_{B'}(r)  - |B'|\right| &= \left| \sum_{\znz} \indi_{B'}(x)\left(e^{-\frac{2\pi i}{N}rx} - 1 \right) \right| \\
        &= \left| \sum_{|x| \leq \frac{1}{2} |B'|} \left(e^{-\frac{2\pi i}{N}rdx} - 1 \right) \right| \\
        &\leq \sum_{|x| \leq \frac{1}{2} |B'|} \left| e^{-\frac{2\pi i}{N}rdx} - 1 \right| \\
        &\leq 2 \sum_{x=0}^{\frac{|B'|}{2}} \frac{2\pi}{N} rdx \\
        &\leq \frac{4\pi l}{N} \sum_{x=0}^{\frac{|B'|}{2}} x \\
        &\leq |B'| \frac{|B'| \pi l}{N} \\
        &\leq \frac{|B'|}{2}
    \end{align*}
    This means
    \begin{align*}
        \left|N \widehat{\indi_B}(r) \right| \geq \frac{|B'|}{2}
    \end{align*}
    The required progression, in which the density of $A$ will be greater than $\left( \delta + \frac{\gamma}{4} \right)$, will be a translation of $B'$, i.e. $B = B' + c$ for some constant $c$.
    
    Define $f_A$ to be the balanced indicator of $A$, i.e.
    \begin{align*}
        f_A(k) = \indi_A(k) - \delta
    \end{align*}
    Notice that the mean of $f_A$ over $[N]$ is $0$. Furthermore, if 
    \begin{align*} 
        \sum_{k=0}^{N-1} f_A(k) \indi_B(k) \geq \kappa |B|
    \end{align*}
    then
    \begin{align*}
        |A \cap B| \geq \left( \delta + \kappa \right)|B|
    \end{align*}
    and vice versa. Here's why.
    \begin{align*}
        \sum_{k=0}^{N-1} f_A(k) \indi_B(k) &= \sum_{k \in B} f_A(k) \\
        &= (1-\delta)|A \cap B| - \delta (|B| - |A \cap B|) \\
        &= |A \cap B| - \delta |B| 
    \end{align*}
    $|A \cap B| - \delta|B|$ will be greater than $\kappa |B|$ iff $|A \cap B| > (\delta + \kappa)|B|$. Now our problem reduces to determining for what $c$, $\sum_{k=0}^{N-1} f_A(k) \indi_{B'}(k-c)$ is greater than or equal to $\frac{\gamma}{4} |B'|$. Define $G(c)$ to be
    \begin{align*}
        G(c) = \sum_{k=0}^{N-1} f_A(k) \indi_{B'}(k-c)
    \end{align*}
    The Fourier transform of $G$ is the following:
    \begin{align*}
        \widehat{G}(r) = N \widehat{f_A}(r) \widehat{\indi_{B'}}(r)
    \end{align*}
    And we know that
    \begin{align*}
        \sum_{[N]} |G(c)| \geq \left| \widehat{G}(r) \right| \geq \frac{1}{2}\gamma N |B'|
    \end{align*}
    And since the mean of $G$ over $[N]$ is $0$
    \begin{align*}
        \sum_{[N]} G(c) + |G(c)| \geq \frac{1}{2}\gamma N |B'|
    \end{align*}
    For some $c_0$
    \begin{align*}
        G(c_0) + |G(c_0)| \geq \frac{1}{2} \gamma |B'|
    \end{align*}
    Hence, $G(c_0) \geq \frac{1}{4}\gamma$
    
    The progression $B = B' + c_0$ is the one that satisfies our requirements.
\end{proof}

\begin{lem} \label{dens:2}
    If $[N]$ has a $\znz$ progression $B$ of length $\frac{\sqrt{N}}{8}$, such that the density of $A$ in $B$ is greater than or equal to $\delta + \frac{\gamma}{4}$, and it is a union of two disjoint $[N]$ progressions, then there exists a progression $P$ in $[N]$ such that $|P| \geq \frac{\gamma}{8} |B|$ such that $A$ has density $\delta + \frac{\gamma}{8}$ in $P$.
\end{lem}

\begin{proof}
    Write $B$ as $P_1 \sqcup P_2$, where $P_1$ and $P_2$ are disjoint $[N]$ progressions, and $|P_1| \leq |P_2|$. If $|P_1| \leq \frac{\gamma}{8} |B|$, then
    \begin{align*}
        |A \cap P_2| &\geq |A \cap B| - |P_1|\\
        &\geq \left( \delta + \frac{\gamma}{4} \right)|B| - P_1 \\
        &\geq \left( \delta + \frac{\gamma}{8} \right)|B| \\
        &\geq \left( \delta + \frac{\gamma}{8} \right)|P_2|
    \end{align*}
    One the other hand, if $|P_1| > \frac{\gamma}{8}$, then one of $A $must have density $\delta + \frac{\gamma}{4}$ on one of $P_1$ or $P_2$.
\end{proof}

With the previous lemmas, we can finally state in full the density incrementation theorem.
\begin{thm}
    Given $0 < \delta < 1$, and $N > \frac{32}{\delta^2}$, and a subset $A$ of $[N]$ of size $\delta N$, if for some non-zero $k$, $\widehat{\indi_A}(k) > \frac{\delta^2}{8}$ or $\left| \left[ \frac{N}{3}, \frac{2N}{3} \right] \cap A \right| < \frac{\delta N}{4}$, then there exists a progression in $[N]$ of length at least $\frac{\delta^2 \sqrt{N}}{512}$ such that the density of $A$ in the progression is at least $\delta + \frac{\delta^2}{64}$.
\end{thm}

\begin{proof}
    Follows from lemmas \ref{dens:1} and \ref{dens:2}.
\end{proof}

\subsection{Iterating the density incrementation argument}
The final step in the proof of Roth's theorem is to increment the density until one reaches a sub progression in which $A$ has density $1$. If the length of that sub progression is greater than $3$, then we're done. After each iteration, the density grows by at least $\frac{\delta^2}{64}$, hence after $k = \frac{64}{\delta^2} \left( 1 - \delta \right)$ steps, the density will be at least $1$. We just need to pick a large enough $N$ such that the sub progression after $k$ steps has at least $3$ elements. We want
\begin{align*}
    \frac{\delta^{2k} N^{\frac{1}{2^k}}}{512^k} &\geq 3 \\
    N &\geq 3^{2^k} \left( \frac{512}{\delta^2} \right)^{2^k \cdot k}
\end{align*} 
Substituting the given value of $k$, we get our lower bound on $N$, and that completes the proof.
\newpage


\bibliography{references}
\bibliographystyle{amsplain}


\end{document}
